"""
=============================================================
Ğ—ĞĞ’Ğ”ĞĞĞĞ¯ 3: Mini RAG (Groq Ğ²ĞµÑ€ÑÑ–Ñ)
Embeddings: sentence-transformers (Ğ±ĞµĞ·ĞºĞ¾ÑˆÑ‚Ğ¾Ğ²Ğ½Ğ¾, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾)
LLM: Groq llama-3.3-70b-versatile (Ğ±ĞµĞ·ĞºĞ¾ÑˆÑ‚Ğ¾Ğ²Ğ½Ğ¾)
=============================================================
pip install groq sentence-transformers python-dotenv
=============================================================
"""

import os
import math
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

from groq import Groq
from sentence_transformers import SentenceTransformer

client = Groq(api_key=os.getenv("GROQ_API_KEY"))
MODEL = "llama-3.3-70b-versatile"
DOC_PATH = Path(__file__).parent / "python_guide.md"

# Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ embeddings (Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ñ‚ÑŒÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· ~90MB)
print("Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–...")
EMBEDDER = SentenceTransformer("all-MiniLM-L6-v2")
print("âœ… Embedding Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°\n")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Ğ—ĞĞ’ĞĞĞ¢ĞĞ–Ğ•ĞĞĞ¯ Ğ¢Ğ Ğ ĞĞ—Ğ‘Ğ˜Ğ¢Ğ¢Ğ¯ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_and_chunk(path: Path, chunk_size: int = 500, overlap: int = 50) -> list[dict]:
    text = path.read_text(encoding="utf-8")

    # Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾ Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»Ğ°Ñ… Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ñ–Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ–Ğ»Ñƒ
    sections = text.split("\n## ")
    chunks = []

    for section in sections:
        if not section.strip():
            continue
        if len(section) <= chunk_size:
            chunks.append({"text": section.strip(), "embedding": None})
        else:
            words = section.split()
            current: list[str] = []
            current_len = 0
            for word in words:
                current.append(word)
                current_len += len(word) + 1
                if current_len >= chunk_size:
                    chunks.append({"text": " ".join(current), "embedding": None})
                    current = current[-(overlap // 10):]
                    current_len = sum(len(w) + 1 for w in current)
            if current:
                chunks.append({"text": " ".join(current), "embedding": None})

    print(f"ğŸ“„ Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾: {len(chunks)} Ñ‡Ğ°Ğ½ĞºÑ–Ğ²")
    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Ğ†ĞĞ”Ğ•ĞšĞ¡ĞĞ¦Ğ†Ğ¯ â€” Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ embeddings
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_index(chunks: list[dict]) -> list[dict]:
    print("ğŸ”„ Ğ†Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ñ–Ñ (sentence-transformers, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾)...")
    texts = [c["text"] for c in chunks]
    embeddings = EMBEDDER.encode(texts, show_progress_bar=False)
    for chunk, emb in zip(chunks, embeddings):
        chunk["embedding"] = emb.tolist()
    print(f"âœ… Ğ†Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ñ–Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° ({len(chunks)} Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ğ²)\n")
    return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. ĞŸĞĞ¨Ğ£Ğš Ğ Ğ•Ğ›Ğ•Ğ’ĞĞĞ¢ĞĞ˜Ğ¥ Ğ¤Ğ ĞĞ“ĞœĞ•ĞĞ¢Ğ†Ğ’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cosine_similarity(a: list[float], b: list[float]) -> float:
    dot = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x ** 2 for x in a))
    norm_b = math.sqrt(sum(x ** 2 for x in b))
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)


def retrieve(query: str, chunks: list[dict], top_k: int = 3) -> list[dict]:
    query_emb = EMBEDDER.encode([query])[0].tolist()
    scored = [
        {"text": c["text"], "score": cosine_similarity(query_emb, c["embedding"])}
        for c in chunks
    ]
    scored.sort(key=lambda x: x["score"], reverse=True)
    return scored[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ†Ğ¯ Ğ’Ğ†Ğ”ĞŸĞĞ’Ğ†Ğ”Ğ•Ğ™
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def answer_without_context(query: str) -> str:
    """Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ğ‘Ğ•Ğ— RAG â€” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ·Ğ½Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–."""
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": "Ğ¢Ğ¸ â€” Ğ¿Ğ¾Ğ¼Ñ–Ñ‡Ğ½Ğ¸Ğº Ğ· Python. Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹ ÑÑ‚Ğ¸ÑĞ»Ğ¾."},
            {"role": "user", "content": query},
        ],
        max_tokens=300,
        temperature=0,
    )
    return (response.choices[0].message.content or "").strip()


def answer_with_context(query: str, relevant_chunks: list[dict]) -> str:
    """Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ğ— RAG ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."""
    context = "\n\n---\n\n".join(
        f"[Score: {c['score']:.3f}]\n{c['text']}"
        for c in relevant_chunks
    )
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "system",
                "content": (
                    "Ğ¢Ğ¸ â€” Ğ¿Ğ¾Ğ¼Ñ–Ñ‡Ğ½Ğ¸Ğº Ğ· Python. Ğ’Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ°Ğ¹ Ğ¢Ğ†Ğ›Ğ¬ĞšĞ˜ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñ– Ğ½Ğ°Ğ´Ğ°Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ. "
                    "Ğ¯ĞºÑ‰Ğ¾ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ½ĞµĞ¼Ğ°Ñ” Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ– â€” Ñ‚Ğ°Ğº Ñ– ÑĞºĞ°Ğ¶Ğ¸."
                ),
            },
            {
                "role": "user",
                "content": f"ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚:\n{context}\n\nĞ—Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ: {query}",
            },
        ],
        max_tokens=400,
        temperature=0,
    )
    return (response.choices[0].message.content or "").strip()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ĞŸĞĞ Ğ†Ğ’ĞĞ¯ĞĞĞ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUERIES = [
    "Ğ©Ğ¾ Ñ‚Ğ°ĞºĞµ Ğ´ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸ Ğ² Python Ñ– ÑĞº Ñ—Ñ… Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒĞ²Ğ°Ñ‚Ğ¸?",
    "Ğ§Ğ¸Ğ¼ Ğ²Ñ–Ğ´Ñ€Ñ–Ğ·Ğ½ÑÑ”Ñ‚ÑŒÑÑ multiprocessing Ğ²Ñ–Ğ´ asyncio?",
    "Ğ¯Ğº Poetry Ğ²Ñ–Ğ´Ñ€Ñ–Ğ·Ğ½ÑÑ”Ñ‚ÑŒÑÑ Ğ²Ñ–Ğ´ pip?",
]


def run_mini_rag():
    print("=" * 65)
    print("Ğ—ĞĞ’Ğ”ĞĞĞĞ¯ 3: MINI RAG")
    print(f"LLM: {MODEL} (Groq)")
    print("Embeddings: sentence-transformers/all-MiniLM-L6-v2 (Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾)")
    print("=" * 65)

    chunks = load_and_chunk(DOC_PATH)
    chunks = build_index(chunks)

    for i, query in enumerate(QUERIES, 1):
        print(f"\n{'=' * 65}")
        print(f"Ğ—ĞĞŸĞ˜Ğ¢ #{i}: {query}")
        print("=" * 65)

        relevant = retrieve(query, chunks, top_k=2)
        print(f"\nĞ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ– Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸:")
        for j, chunk in enumerate(relevant, 1):
            preview = chunk["text"][:100].replace("\n", " ")
            print(f"  [{j}] Score={chunk['score']:.4f} | {preview}...")

        print(f"\nĞ‘Ğ•Ğ— ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢Ğ£:")
        print(answer_without_context(query))

        print(f"\nĞ— ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢ĞĞœ (RAG):")
        print(answer_with_context(query, relevant))

    print(f"\n{'=' * 65}")
    print("ĞĞĞĞ›Ğ†Ğ— Ğ Ğ†Ğ—ĞĞ˜Ğ¦Ğ†:")
    print("=" * 65)
    print("""
Ğ‘Ğ•Ğ— ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ:              Ğ— ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (RAG):
â€¢ Ğ—Ğ°Ğ³Ğ°Ğ»ÑŒĞ½Ñ– Ğ·Ğ½Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–    â€¢ Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ñ–Ñ‡Ğ½Ğ° Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°
â€¢ ĞœĞ¾Ğ¶Ğµ Ğ³Ğ°Ğ»ÑÑ†Ğ¸Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸         â€¢ Ğ—Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ° Ñƒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ‚ĞµĞºÑÑ‚Ñ–
â€¢ ĞĞµ Ğ·Ğ½Ğ°Ñ” Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ   â€¢ Ğ¢Ğ¾Ñ‡Ğ½Ñ– Ğ´ĞµÑ‚Ğ°Ğ»Ñ– Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ—
â€¢ Ğ¨Ğ²Ğ¸Ğ´ÑˆĞ¸Ğ¹                   â€¢ ĞŸĞ¾ÑĞ¸Ğ»Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ğ¶ĞµÑ€ĞµĞ»Ğ¾

Ğ’Ğ¸ÑĞ½Ğ¾Ğ²Ğ¾Ğº: RAG Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾ĞºÑ€Ğ°Ñ‰ÑƒÑ” Ñ‚Ğ¾Ñ‡Ğ½Ñ–ÑÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ñ–Ñ‡Ğ½Ğ¸Ñ…
Ğ¿Ğ¸Ñ‚Ğ°Ğ½ÑŒ Ğ¿Ñ€Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚/Ğ¿Ñ€Ğ¾ĞµĞºÑ‚.
""")


if __name__ == "__main__":
    run_mini_rag()
