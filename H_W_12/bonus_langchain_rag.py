"""
=============================================================
Ð‘ÐžÐÐ£Ð¡: LangChain RAG + Structured Output (Groq Ð²ÐµÑ€ÑÑ–Ñ)
LLM: Groq llama-3.3-70b-versatile
Embeddings: HuggingFace all-MiniLM-L6-v2 (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾, Ð±ÐµÐ·ÐºÐ¾ÑˆÑ‚Ð¾Ð²Ð½Ð¾)
=============================================================
pip install langchain langchain-groq langchain-community
pip install langchain-text-splitters langchain-huggingface
pip install faiss-cpu sentence-transformers python-dotenv
=============================================================
"""

import os
import json
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

load_dotenv()

from pydantic import BaseModel, Field, SecretStr
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYDANTIC Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð Ð’Ð†Ð”ÐŸÐžÐ’Ð†Ð”Ð†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RAGResponse(BaseModel):
    answer: str = Field(description="Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð½Ð° Ð·Ð°Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ")
    confidence: float = Field(description="Ð’Ð¿ÐµÐ²Ð½ÐµÐ½Ñ–ÑÑ‚ÑŒ Ð²Ñ–Ð´ 0.0 Ð´Ð¾ 1.0", ge=0.0, le=1.0)
    sources_used: list[str] = Field(description="ÐšÐ»ÑŽÑ‡Ð¾Ð²Ñ– Ñ‚ÐµÐ·Ð¸ Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°")
    answer_found_in_docs: bool = Field(description="True ÑÐºÑ‰Ð¾ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…")
    follow_up_questions: Optional[list[str]] = Field(
        default=None, description="3 Ð¿Ð¾Ð²'ÑÐ·Ð°Ð½Ð¸Ñ… Ð·Ð°Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ"
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Ð—ÐÐ’ÐÐÐ¢ÐÐ–Ð•ÐÐÐ¯ Ð¢Ð Ð†ÐÐ”Ð•ÐšÐ¡ÐÐ¦Ð†Ð¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_vectorstore(doc_path: str) -> FAISS:
    text = Path(doc_path).read_text(encoding="utf-8")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=80,
        separators=["\n## ", "\n### ", "\n\n", "\n", " "],
    )
    docs = splitter.create_documents(
        texts=[text],
        metadatas=[{"source": doc_path}]
    )
    print(f"ðŸ“„ Ð Ð¾Ð·Ð±Ð¸Ñ‚Ð¾ Ð½Ð° {len(docs)} Ñ‡Ð°Ð½ÐºÑ–Ð²")

    # Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ– embeddings â€” ÐÐ• Ð¿Ð¾Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑŒ API ÐºÐ»ÑŽÑ‡Ð°
    print("ðŸ”„ Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ HuggingFace embeddings (Ð¿ÐµÑ€ÑˆÐ¸Ð¹ Ñ€Ð°Ð· ~90MB)...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vectorstore = FAISS.from_documents(docs, embeddings)
    print("âœ… FAISS Ñ–Ð½Ð´ÐµÐºÑ Ð¿Ð¾Ð±ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¾\n")
    return vectorstore


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. RAG CHAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_rag_chain(vectorstore: FAISS):
    llm = ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0,
        api_key=SecretStr(os.getenv("GROQ_API_KEY") or ""),
    )

    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3},
    )

    parser = JsonOutputParser(pydantic_object=RAGResponse)

    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            "Ð¢Ð¸ â€” Python-ÐµÐºÑÐ¿ÐµÑ€Ñ‚. Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– Ð½Ð°Ð´Ð°Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ñƒ.\n"
            "Ð¯ÐºÑ‰Ð¾ Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– Ð½ÐµÐ¼Ð°Ñ” â€” Ð²ÐºÐ°Ð¶Ð¸ Ñ†Ðµ Ñ‡ÐµÑÐ½Ð¾.\n"
            "ÐŸÐ¾Ð²ÐµÑ€Ð½Ð¸ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Ð²Ð°Ð»Ñ–Ð´Ð½Ð¸Ð¹ JSON Ð±ÐµÐ· ```json Ñ‚Ð° Ð±ÐµÐ· Ð·Ð°Ð¹Ð²Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ.\n\n"
            "{format_instructions}",
        ),
        (
            "human",
            "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context}\n\nÐ—Ð°Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ: {question}",
        ),
    ])
    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    def rag_chain(query: str) -> RAGResponse:
        relevant_docs: list[Document] = retriever.invoke(query)
        context = "\n\n---\n\n".join(d.page_content for d in relevant_docs)

        messages = prompt.format_messages(context=context, question=query)
        raw_response = llm.invoke(messages)

        content = raw_response.content
        if isinstance(content, list):
            content = " ".join(
                c["text"] if isinstance(c, dict) else str(c) for c in content
            )
        content = str(content).strip()

        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        content = content.strip()

        parsed = json.loads(content)
        return RAGResponse(**parsed)

    return rag_chain


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Ð”Ð•ÐœÐžÐÐ¡Ð¢Ð ÐÐ¦Ð†Ð¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

QUERIES = [
    "Ð¯Ðº Ð¿Ñ€Ð°Ñ†ÑŽÑ” asyncio Ñ– Ñ‡Ð¸Ð¼ Ð²Ñ–Ð´Ñ€Ñ–Ð·Ð½ÑÑ”Ñ‚ÑŒÑÑ Ð²Ñ–Ð´ multiprocessing?",
    "Ð¯ÐºÑ– Ð±Ñ–Ð±Ð»Ñ–Ð¾Ñ‚ÐµÐºÐ¸ Python Ð¿Ñ–Ð´Ñ…Ð¾Ð´ÑÑ‚ÑŒ Ð´Ð»Ñ Ð¼Ð°ÑˆÐ¸Ð½Ð½Ð¾Ð³Ð¾ Ð½Ð°Ð²Ñ‡Ð°Ð½Ð½Ñ?",
    "Ð¯Ðº Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ñ‚Ð¸ pytest Ð´Ð»Ñ Ñ‚ÐµÑÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ?",
]

DOC_PATH = str(Path(__file__).parent / "python_guide.md")


def run_langchain_rag():
    print("=" * 65)
    print("Ð‘ÐžÐÐ£Ð¡: LangChain RAG + Structured Output")
    print("LLM: Groq llama-3.3-70b | Embeddings: HuggingFace (Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾)")
    print("=" * 65)

    vectorstore = build_vectorstore(DOC_PATH)
    rag_chain = build_rag_chain(vectorstore)

    for i, query in enumerate(QUERIES, 1):
        print(f"\n{'=' * 65}")
        print(f"Ð—ÐÐŸÐ˜Ð¢ #{i}: {query}")
        print("=" * 65)

        response: RAGResponse = rag_chain(query)

        print(f"\nÐ’Ð†Ð”ÐŸÐžÐ’Ð†Ð”Ð¬:\n  {response.answer}")
        print(f"\nÐ’Ð¿ÐµÐ²Ð½ÐµÐ½Ñ–ÑÑ‚ÑŒ:   {response.confidence:.0%}")
        print(f"Ð— Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ–Ð²:  {'Ð¢ÐÐš' if response.answer_found_in_docs else 'ÐÐ†'}")
        print(f"\nÐ’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ñ– Ñ‚ÐµÐ·Ð¸:")
        for src in response.sources_used:
            print(f"  â€¢ {src}")
        if response.follow_up_questions:
            print(f"\nÐÐ°ÑÑ‚ÑƒÐ¿Ð½Ñ– Ð·Ð°Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ:")
            for fq in response.follow_up_questions:
                print(f"  â†’ {fq}")

    print(f"\n{'=' * 65}")
    print("LangChain pipeline:")
    print("  Query -> HuggingFace Embeddings -> FAISS -> Top-K Docs")
    print("  -> ChatPromptTemplate -> Groq LLM -> JsonOutputParser -> RAGResponse")


if __name__ == "__main__":
    run_langchain_rag()
